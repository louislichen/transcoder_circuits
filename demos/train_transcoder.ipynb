{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3a1b26",
   "metadata": {},
   "source": [
    "# Train Transcoders on LLMs automatically\n",
    "\n",
    "The origin training code in Transcoder is written for the specific layer in the model. To conviniently train our model, we rewrite the training code.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2e71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformer_lens import HookedTransformer\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "import json\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "sys.path.append(os.path.abspath(\"..\")) \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from sae_training.config import LanguageModelSAERunnerConfig\n",
    "from sae_training.utils import LMSparseAutoencoderSessionloader\n",
    "from sae_training.train_sae_on_language_model import train_sae_on_language_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace74821",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "First, TransformerLens doesn't support all the models in Huggingface. So before training, choose a model you want [here](https://transformerlensorg.github.io/TransformerLens/generated/model_properties_table.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8db39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078588c6",
   "metadata": {},
   "source": [
    "Now, we download the pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "158b3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:7\"\n",
    "# model = HookedTransformer.from_pretrained(model_name=model_name, device=device, n_devices=8, move_to_device=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339b7bfe",
   "metadata": {},
   "source": [
    "So far, we already have the model. Let's first test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "643935b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# model.generate(\"5*(10+2*3). Let's think step by step.\", max_new_tokens=50, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad299be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_layer_cfg(dataset_path: str, model_name: str, layer_idx: int, d_model: int, out_dir: str,\n",
    "                   expansion_factor: int, lr: float, l1_coeff: float, train_batch_size: int, context_size: int,\n",
    "                   device: torch.device, dtype: torch.dtype) -> LanguageModelSAERunnerConfig:\n",
    "    \"\"\"\n",
    "    build RunnerConfig for a specific layer\n",
    "    \"\"\"\n",
    "    layer_ckpt_dir = Path(out_dir) / f\"layer_{layer_idx:02d}\"\n",
    "    layer_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cfg = LanguageModelSAERunnerConfig(\n",
    "        hook_point=f\"blocks.{layer_idx}.ln2.hook_normalized\",  # input of MLP\n",
    "        hook_point_layer=layer_idx,\n",
    "        d_in=d_model,\n",
    "        dataset_path=dataset_path,\n",
    "        is_dataset_tokenized=False,\n",
    "        model_name=model_name,\n",
    "\n",
    "        is_transcoder=True,\n",
    "        out_hook_point=f\"blocks.{layer_idx}.hook_mlp_out\",\n",
    "        out_hook_point_layer=layer_idx,\n",
    "        d_out=d_model,\n",
    "\n",
    "        expansion_factor=expansion_factor,\n",
    "        b_dec_init_method=\"mean\",\n",
    "\n",
    "        # Training Parameters\n",
    "        lr=lr,\n",
    "        l1_coefficient=l1_coeff,\n",
    "        lr_scheduler_name=\"constantwithwarmup\",\n",
    "        train_batch_size=train_batch_size,\n",
    "        context_size=context_size,\n",
    "        lr_warm_up_steps=5_000,\n",
    "\n",
    "        # Activation Store Parameters\n",
    "        n_batches_in_buffer=2,\n",
    "        total_training_tokens=1_000_000 * 60,\n",
    "        store_batch_size=4,\n",
    "\n",
    "        # Dead Neurons and Sparsity\n",
    "        use_ghost_grads=True,\n",
    "        feature_sampling_method=None,\n",
    "        feature_sampling_window=1000,\n",
    "        resample_batches=512,\n",
    "        dead_feature_window=5000,\n",
    "        dead_feature_threshold=1e-8,\n",
    "\n",
    "        log_to_wandb=False,\n",
    "        use_tqdm=True,\n",
    "        device=device,\n",
    "        seed=42,\n",
    "        n_checkpoints=3,\n",
    "        checkpoint_path=str(layer_ckpt_dir),\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1befed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_layer(cfg: LanguageModelSAERunnerConfig):\n",
    "    print(f\"[Layer {cfg.hook_point_layer}] Start training: lr={cfg.lr} l1={cfg.l1_coefficient} Checkpoint dir: {cfg.checkpoint_path}\")\n",
    "\n",
    "    loader = LMSparseAutoencoderSessionloader(cfg)\n",
    "    model, sparse_autoencoder, activations_loader = loader.load_session()\n",
    "\n",
    "    sparse_autoencoder = train_sae_on_language_model(\n",
    "        model, sparse_autoencoder, activations_loader,\n",
    "        n_checkpoints=cfg.n_checkpoints,\n",
    "        batch_size=cfg.train_batch_size,\n",
    "        feature_sampling_method=cfg.feature_sampling_method,\n",
    "        feature_sampling_window=cfg.feature_sampling_window,\n",
    "        feature_reinit_scale=cfg.feature_reinit_scale,\n",
    "        dead_feature_threshold=cfg.dead_feature_threshold,\n",
    "        dead_feature_window=cfg.dead_feature_window,\n",
    "        use_wandb=cfg.log_to_wandb,\n",
    "        wandb_log_frequency=cfg.wandb_log_frequency\n",
    "    )\n",
    "\n",
    "    final_path = Path(cfg.checkpoint_path) / f\"final_{sparse_autoencoder.get_name()}.pt\"\n",
    "    sparse_autoencoder.save_model(str(final_path))\n",
    "    print(f\"[Layer {cfg.hook_point_layer}] Saved: {final_path}\")\n",
    "\n",
    "    del model, sparse_autoencoder, activations_loader\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9e25668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_path_exists(ckpt_dir: Path) -> bool:\n",
    "    \"\"\"\n",
    "    check whether final checkpoint exists\n",
    "    \"\"\"\n",
    "    if not ckpt_dir.exists():\n",
    "        return False\n",
    "    for p in ckpt_dir.glob(\"final_*.pt\"):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6342f3",
   "metadata": {},
   "source": [
    "Now, define the params for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f7fc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0004                                                 # learning rate\n",
    "l1_coeff = 0.00014                                          # l1 sparsity regularization coefficient\n",
    "expansion_factor = 16                                       # expansion factor（default 32）\n",
    "dtype = torch.float32                                       # dtype\n",
    "\n",
    "train_batch = 32                                            # training batch size\n",
    "train_epoch = 200                                           # training epoch\n",
    "context_size = 512                                          # context size\n",
    "\n",
    "dataset_path = \"cerebras/SlimPajama-627B\"                   # dataset path\n",
    "checkpoint_dir = f\"./{model_name}_checkpoints\"              # checkpoint directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fab0c067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: meta-llama/Llama-3.1-8B-Instruct | d_model=4096 | n_layers=32\n",
      "Run name: 65536-L1-0.00014-LR-0.0004-Tokens-6.000e+07\n",
      "n_tokens_per_buffer (millions): 0.004096\n",
      "Lower bound: n_contexts_per_buffer (millions): 8e-06\n",
      "Total training steps: 1875000\n",
      "Total wandb updates: 187500\n",
      "n_tokens_per_feature_sampling_window (millions): 16.384\n",
      "n_tokens_per_dead_feature_window (millions): 81.92\n",
      "Using Ghost Grads.\n",
      "We will reset the sparsity calculation 1875 times.\n",
      "Number of tokens when resampling: 2048\n",
      "Number tokens in sparsity calculation window: 3.20e+04\n",
      "[Layer 21] Start training: lr=0.0004 l1=0.00014 Checkpoint dir: meta-llama/Llama-3.1-8B-Instruct_checkpoints/layer_21/i2c3nzsc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f0af4e112344f4a618beb3242f1947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Llama-3.1-8B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892b1e56a1be4e7fa675337d7e1c0286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/59166 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ecf271b06a44232bc5e211a527d7cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31428 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd81c61c91644e5f982476b765054918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/31411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is not tokenized! Updating config.\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 63.996585845947266\n",
      "New distances: 58.10400390625\n",
      "Reinitializing b_dec with mean of activations\n",
      "Previous distances: 6.712014198303223\n",
      "New distances: 6.45009183883667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13516| MSE Loss 0.001544 | L1 0.000000:   1%|          | 432512/60000000 [20:47<60:44:29, 272.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Early Stop] Stop at step 13517, best loss=0.001301\n",
      "Saved model to meta-llama/Llama-3.1-8B-Instruct_checkpoints/layer_21/i2c3nzsc/final_sparse_autoencoder_meta-llama/Llama-3.1-8B-Instruct_blocks.21.ln2.hook_normalized_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13516| MSE Loss 0.001544 | L1 0.000000:   1%|          | 432544/60000000 [20:59<60:44:29, 272.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to meta-llama/Llama-3.1-8B-Instruct_checkpoints/layer_21/i2c3nzsc/final_sparse_autoencoder_meta-llama/Llama-3.1-8B-Instruct_blocks.21.ln2.hook_normalized_65536.pt\n",
      "[Layer 21] Saved: meta-llama/Llama-3.1-8B-Instruct_checkpoints/layer_21/i2c3nzsc/final_sparse_autoencoder_meta-llama/Llama-3.1-8B-Instruct_blocks.21.ln2.hook_normalized_65536.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13516| MSE Loss 0.001544 | L1 0.000000:   1%|          | 432544/60000000 [20:59<48:11:31, 343.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "d_model, n_layers = 4096, 32\n",
    "print(f\"Model: {model_name} | d_model={d_model} | n_layers={n_layers}\")\n",
    "meta_path = Path(checkpoint_dir) / \"meta.json\"\n",
    "json.dump({\"model_name\": model_name, \"d_model\": d_model, \"n_layers\": n_layers}, open(meta_path, \"w\"))\n",
    "\n",
    "layer_idx = 21\n",
    "layer_dir = Path(checkpoint_dir)/f\"layer_{layer_idx:02d}\"\n",
    "cfg = make_layer_cfg(dataset_path, model_name, layer_idx, d_model, checkpoint_dir, expansion_factor, lr, l1_coeff, train_batch, context_size,\n",
    "                        device, dtype)\n",
    "\n",
    "train_one_layer(cfg)\n",
    "\n",
    "# layer_range = list(range(n_layers))\n",
    "\n",
    "# for layer_idx in layer_range:\n",
    "#     layer_dir = Path(checkpoint_dir)/f\"layer_{layer_idx:02d}\"\n",
    "#     if final_path_exists(layer_dir):\n",
    "#         print(f\"[Layer {layer_idx}] final checkpoint exists, skip.\")\n",
    "#         continue\n",
    "#     cfg = make_layer_cfg(dataset_path, model_name, layer_idx, d_model, checkpoint_dir, expansion_factor, lr, l1_coeff, train_batch, context_size,\n",
    "#                          device, dtype)\n",
    "    \n",
    "#     train_one_layer(cfg)\n",
    "\n",
    "\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4454e2e4",
   "metadata": {},
   "source": [
    "P.S. OOM is common since the sparsity feature dimension is very large. I handled this by decreasing the sparsity factor, as this is only a demo for validation rather than the formal version. If you prefer not to do so, consider using FSDP or other techniques to address it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
